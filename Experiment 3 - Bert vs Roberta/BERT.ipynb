{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2ef71-74da-4925-8446-ab087494f0b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "\n",
    "# 1. Seed setzen für Reproduzierbarkeit\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 2. CoNLL-Dateien laden\n",
    "def load_conll_data(file_path):\n",
    "    tokens, ner_tags = [], []\n",
    "    all_tokens, all_tags = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if tokens:\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_tags.append(ner_tags)\n",
    "                tokens, ner_tags = [], []\n",
    "            else:\n",
    "                splits = line.strip().split()\n",
    "                tokens.append(splits[0])\n",
    "                ner_tags.append(splits[-1])\n",
    "    if tokens:\n",
    "        all_tokens.append(tokens)\n",
    "        all_tags.append(ner_tags)\n",
    "    return all_tokens, all_tags\n",
    "\n",
    "# 3. Lade alle CoNLL-Dateien\n",
    "data_dir = \"/bachelorarbeit-ner/data/annotated\"\n",
    "all_tokens, all_tags = [], []\n",
    "for file_name in sorted(os.listdir(data_dir)):\n",
    "    if file_name.endswith(\".conll\"):\n",
    "        tokens, tags = load_conll_data(os.path.join(data_dir, file_name))\n",
    "        all_tokens.extend(tokens)\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "# 4. Aufteilen in train/val/test\n",
    "train_tokens, temp_tokens, train_tags, temp_tags = train_test_split(all_tokens, all_tags, test_size=0.3, random_state=42)\n",
    "val_tokens, test_tokens, val_tags, test_tags = train_test_split(temp_tokens, temp_tags, test_size=0.5, random_state=42)\n",
    "\n",
    "# 5. Tag-Mapping\n",
    "unique_tags = sorted(list(set(tag for doc in all_tags for tag in doc)))\n",
    "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "\n",
    "def create_dataset(tokens, tags):\n",
    "    data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": [[tag2id[tag] for tag in tag_seq] for tag_seq in tags]\n",
    "    }\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": create_dataset(train_tokens, train_tags),\n",
    "    \"validation\": create_dataset(val_tokens, val_tags),\n",
    "    \"test\": create_dataset(test_tokens, test_tags)\n",
    "})\n",
    "\n",
    "# 6. Tokenisierung\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\", \"ner_tags\"]\n",
    ")\n",
    "\n",
    "# 7. Metriken\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "# 8. Trainings-Schleife für 10 Seeds\n",
    "all_results = []\n",
    "for seed in range(1, 11):\n",
    "    print(f\"\\n===== Training with seed {seed} =====\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(tag2id))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/bert_seed_{seed}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"no\",\n",
    "        logging_dir=f\"./logs/bert_seed_{seed}\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    eval_results[\"seed\"] = seed\n",
    "    all_results.append(eval_results)\n",
    "\n",
    "# 9. Ergebnisse speichern\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_path = \"/bachelorarbeit-ner/results/bert_base_eval_10runs.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nGesamtergebnis gespeichert unter: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067e577-2438-4d0e-876c-bfc4b35ede8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
