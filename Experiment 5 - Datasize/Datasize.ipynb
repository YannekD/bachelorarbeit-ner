{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d3958-93ca-4336-9dc0-ccba7a90f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Seed-Funktion für Reproduzierbarkeit\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "# 2. CoNLL-Daten laden\n",
    "def load_conll_data(file_path):\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if tokens:\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_tags.append(ner_tags)\n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "            else:\n",
    "                splits = line.strip().split()\n",
    "                tokens.append(splits[0])\n",
    "                ner_tags.append(splits[-1])\n",
    "    if tokens:\n",
    "        all_tokens.append(tokens)\n",
    "        all_tags.append(ner_tags)\n",
    "    \n",
    "    return all_tokens, all_tags\n",
    "\n",
    "data_dir = r\"C:\\Users\\Yannek Dirksen\\Desktop\\Uni\\Bachelorarbeit\\For Real\\Annotierte Pamphletes\"\n",
    "all_tokens, all_tags = [], []\n",
    "\n",
    "for file_name in sorted(os.listdir(data_dir)):\n",
    "    if file_name.endswith(\".conll\"):\n",
    "        tokens, tags = load_conll_data(os.path.join(data_dir, file_name))\n",
    "        all_tokens.extend(tokens)\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "# 3. Aufteilen in Trainings-, Validierungs- und Testdaten (70/15/15)\n",
    "train_tokens, temp_tokens, train_tags, temp_tags = train_test_split(all_tokens, all_tags, test_size=0.3, random_state=GLOBAL_SEED)\n",
    "val_tokens, test_tokens, val_tags, test_tags = train_test_split(temp_tokens, temp_tags, test_size=0.5, random_state=GLOBAL_SEED)\n",
    "\n",
    "# 4. Mapping von NER-Tags auf IDs\n",
    "unique_tags = list(set(tag for doc in all_tags for tag in doc))\n",
    "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "\n",
    "def create_dataset(tokens, tags):\n",
    "    data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": [[tag2id[tag] for tag in tag_seq] for tag_seq in tags]\n",
    "    }\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "train_dataset = create_dataset(train_tokens, train_tags)\n",
    "val_dataset = create_dataset(val_tokens, val_tags)\n",
    "test_dataset = create_dataset(test_tokens, test_tags)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# 5. Tokenisierung und Label-Ausrichtung\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\", \"ner_tags\"]\n",
    ")\n",
    "\n",
    "# 6. Trainingsargumente (gleich für alle Durchläufe)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=GLOBAL_SEED  # Dieser Seed wird aber später für jeden Run individuell angepasst\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [id2tag[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[lab] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    flat_metrics = {\n",
    "        \"eval_overall_precision\": results[\"overall_precision\"],\n",
    "        \"eval_overall_recall\": results[\"overall_recall\"],\n",
    "        \"eval_overall_f1\": results[\"overall_f1\"],\n",
    "        \"eval_overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    return flat_metrics\n",
    "\n",
    "# 7. Experiment: Datenanteils-Experiment mit mehrfachen Durchläufen und individuellen Seeds\n",
    "fractions = [i / 10 for i in range(1, 11)]  # 0.1, 0.2, ..., 1.0\n",
    "results_dict = {}\n",
    "num_runs = 10  # Anzahl der Wiederholungen pro Anteil; kann auf 10 erhöht werden\n",
    "\n",
    "for frac in fractions:\n",
    "    frac_str = f\"{int(frac*100)}%\"\n",
    "    f1_scores = []\n",
    "    print(f\"\\n--- Training mit {frac_str} der Trainingsdaten ---\")\n",
    "    subset_size = int(len(tokenized_datasets[\"train\"]) * frac)\n",
    "    \n",
    "    # Für jeden Datenanteil mehrere Durchläufe\n",
    "    for run in range(num_runs):\n",
    "        # Setze individuellen Seed für den Run (GLOBAL_SEED + run)\n",
    "        current_seed = GLOBAL_SEED + run\n",
    "        set_seed(current_seed)\n",
    "        \n",
    "        # Aktualisiere den Seed in den Trainingsargumenten\n",
    "        current_training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{frac_str}/run_{run+1}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            logging_dir=f'./logs/{frac_str}/run_{run+1}',\n",
    "            logging_steps=10,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_overall_f1\",\n",
    "            greater_is_better=True,\n",
    "            seed=current_seed\n",
    "        )\n",
    "        \n",
    "        # Stelle sicher, dass der Subset für jeden Run gleich ist\n",
    "        subset_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=GLOBAL_SEED).select(range(subset_size))\n",
    "        \n",
    "        # Lade ein frisches Modell für jeden Run\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(unique_tags))\n",
    "        \n",
    "        # Initialisiere den Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=current_training_args,\n",
    "            train_dataset=subset_train_dataset,\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluation auf dem Testdatensatz\n",
    "        test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "        f1_score = test_results[\"eval_overall_f1\"]\n",
    "        f1_scores.append(f1_score)\n",
    "        print(f\"Run {run+1}: F1 Score = {f1_score:.4f}\")\n",
    "    \n",
    "    # Berechne Mittelwert und Standardabweichung der F1-Scores für den aktuellen Datenanteil\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    results_dict[frac_str] = (mean_f1, std_f1)\n",
    "    print(f\"Ergebnisse für {frac_str}: Mean F1 = {mean_f1:.4f}, Std = {std_f1:.4f}\")\n",
    "\n",
    "print(\"\\n--- Zusammenfassung der Ergebnisse ---\")\n",
    "for data_percent, (mean_f1, std_f1) in results_dict.items():\n",
    "    print(f\"{data_percent}: Mean F1 Score = {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
